{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6fd5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the context private? Yes\n",
      "Automatic relinearization is: on\n",
      "Automatic rescaling is: on\n",
      "Automatic modulus switching is: on\n"
     ]
    }
   ],
   "source": [
    "from ET_ContextGenerator import key_context as context\n",
    "import os\n",
    "#22 elements took "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828862b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.getcwd()\n",
    "if dir.split(\"/\")[-3] == \"codebase\":\n",
    "    os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6ce342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tenseal as ts\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3858f76a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fef6cf9cb30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cba6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ef2659",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# importing module\n",
    "import logging\n",
    "\n",
    "\n",
    "# Create and configure logger\n",
    "logging.basicConfig(\n",
    "    filename=\"encrypted_evaluation.log\", format=\"%(asctime)s %(message)s\", filemode=\"a\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c6b6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Due to the nature of encrypted data we can't do conditional analysis on the inputs, we are also limited to the basic mathametical arithmatic, thus this limits the activation functions we can use. For this example I am going to use an approximation to the Sigmoid Activation,  I am going to minimise the number of polynomial arithmetic to minimise Loss of value whilst still maintain some accuracy for the sigmoid value, I will be using this approximation that''l range the sigmoid to [-5, 5]. Which is done similliarly in this research paper. https://eprint.iacr.org/2018/462.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6055b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLinReg:\n",
    "    def __init__(self, torch_lr):\n",
    "        self.weight = torch_lr.linear.weight.data.tolist()[0]\n",
    "        self.bias = torch_lr.linear.bias.data.tolist()\n",
    "        # we accumulate gradients and counts the number of iterations\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "\n",
    "    def forward(self, enc_x):\n",
    "        enc_out = enc_x.dot(self.weight) + self.bias\n",
    "        # enc_out = EncryptedLinReg.sigmoid(enc_out)\n",
    "        enc_out = EncryptedLinReg.square(enc_out)\n",
    "        return enc_out\n",
    "\n",
    "    def backward(self, enc_x, enc_out, enc_y):\n",
    "        out_minus_y = enc_out - enc_y\n",
    "        print(\n",
    "            f\"Loss for current iteration is {sum(abs(number) for number in out_minus_y.decrypt())}\"\n",
    "        )\n",
    "        self._delta_w += enc_x * out_minus_y\n",
    "        self._delta_b += out_minus_y\n",
    "        self._count += 1\n",
    "\n",
    "    def update_parameters(self):\n",
    "        if self._count == 0:\n",
    "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
    "        # update weights\n",
    "        # We use a small regularization term to keep the output\n",
    "        # of the linear layer in the range of the sigmoid approximation\n",
    "        self.weight -= (\n",
    "            self._delta_w * (1 / self._count) + self.weight \n",
    "        ) \n",
    "        self.bias -= self._delta_b * (1 / self._count)\n",
    "        # reset gradient accumulators and iterations count\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def square(enc_x):\n",
    "        return enc_x * enc_x\n",
    "\n",
    "    def plain_accuracy(self, x_test, y_test):\n",
    "        # evaluate accuracy of the model on\n",
    "        # the plain (x_test, y_test) dataset\n",
    "        w = torch.tensor(self.weight)\n",
    "        b = torch.tensor(self.bias)\n",
    "        out = (x_test.matmul(w) + b).reshape(-1, 1)\n",
    "        with torch.no_grad():  # no need to calculate gradients for testing\n",
    "            y_pred_experience = torch.round(\n",
    "                out, decimals=-1\n",
    "            )  # nearest year experience rounded to whole number\n",
    "            y_real_rounded = torch.round(\n",
    "                y_test, decimals=-1\n",
    "            )  # real salary (rounded unit: the thousands) LHS of the decimal\n",
    "            accuracy = torch.eq(y_pred_experience, y_real_rounded)\n",
    "        return torch.sum(accuracy).item() / len(accuracy) * 100\n",
    "\n",
    "    def encrypt(self, context):\n",
    "        # self.weight = ts.ckks_tensor(context, self.weight)\n",
    "        # self.bias = ts.ckks_tensor(context, self.bias)\n",
    "        self.weight = ts.ckks_vector(context, self.weight)\n",
    "        self.bias = ts.ckks_vector(context, self.bias)\n",
    "\n",
    "    def decrypt(self):\n",
    "        self.weight = self.weight.decrypt()\n",
    "        self.bias = self.bias.decrypt()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d155a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "context = context\n",
    "\n",
    "df = pd.read_csv(\"./LinearRegression/Data/Custom_Salary_Data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b7cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(df):\n",
    "    training_data = df.sample(frac=0.5, random_state=25)  #\n",
    "    testing_data = df.drop(training_data.index)\n",
    "    y_train, x_train = (\n",
    "        training_data[\"YearsExperience\"].to_numpy(),\n",
    "        training_data[\"Salary\"].to_numpy(),\n",
    "    )\n",
    "    y_test, x_test = (\n",
    "        testing_data[\"YearsExperience\"].to_numpy(),\n",
    "        testing_data[\"Salary\"].to_numpy(),\n",
    "    )\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b77812",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption of the training_set took 0.10849690437316895 seconds\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = test_train_split(df)\n",
    "\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "x_test = x_test.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train).double().requires_grad_()\n",
    "y_train_tensor = torch.from_numpy(y_train).double().requires_grad_()\n",
    "\n",
    "x_test_tensor = torch.from_numpy(x_test).double()\n",
    "y_test_tensor = torch.from_numpy(y_test).double()\n",
    "t_start = time.time()\n",
    "# enc_x_train = ts.ckks_tensor(context, x_train)\n",
    "# enc_y_train = ts.ckks_tensor(context, y_train)\n",
    "\n",
    "########################################################################################################################\n",
    "enc_x_train = ts.ckks_vector(context, x_train.flatten())\n",
    "enc_y_train = ts.ckks_vector(context, y_train.flatten())\n",
    "########################################################################################################################\n",
    "t_end = time.time()\n",
    "print(f\"Encryption of the training_set took {(t_end - t_start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eadc798",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Use to build a good activation function\\nnormal_dist = lambda x, mean, var: np.exp(-np.square(x - mean) / (2 * var)) / np.sqrt(\\n    2 * np.pi * var\\n)\\n\\n\\ndef plot_normal_dist(mean, var, rmin=-17807, rmax=12193):\\n    x = np.arange(rmin, rmax, 0.1)\\n    y = normal_dist(x, mean, var)\\n    fig = plt.plot(x, y)\\n\\n\\n# plain distribution\\nlin_reg_model = LinearRegressionModel(1, 1)\\nlin_reg_model = lin_reg_model.double()\\ndata = lin_reg_model(x_train_tensor)\\nmean, var = map(float, [data.mean(), data.std() ** 2])\\nprint(mean)\\nplot_normal_dist(mean, var)\\nprint(\"Distribution on plain data:\")\\nplt.show()\\n\\n\\n# +\\n# encrypted distribution\\ndef encrypted_out_distribution(eelr, enc_x_test):\\n    w = eelr.weight\\n    b = eelr.bias\\n    data = []\\n\\n    enc_out = enc_x_test.dot(w) + b\\n    data.append(enc_out.decrypt())\\n    print(data)\\n    data = torch.tensor(data)\\n    mean, var = map(float, [data.mean(), data.std() ** 2])\\n    plot_normal_dist(mean, var)\\n    print(\"Distribution on encrypted data:\")\\n    plt.show()\\n\\n\\neelr = EncryptedLinReg(lin_reg_model)\\neelr.encrypt(context)\\nencrypted_out_distribution(eelr, enc_x_train)\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Use to build a good activation function\n",
    "normal_dist = lambda x, mean, var: np.exp(-np.square(x - mean) / (2 * var)) / np.sqrt(\n",
    "    2 * np.pi * var\n",
    ")\n",
    "\n",
    "\n",
    "def plot_normal_dist(mean, var, rmin=-17807, rmax=12193):\n",
    "    x = np.arange(rmin, rmax, 0.1)\n",
    "    y = normal_dist(x, mean, var)\n",
    "    fig = plt.plot(x, y)\n",
    "\n",
    "\n",
    "# plain distribution\n",
    "lin_reg_model = LinearRegressionModel(1, 1)\n",
    "lin_reg_model = lin_reg_model.double()\n",
    "data = lin_reg_model(x_train_tensor)\n",
    "mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "print(mean)\n",
    "plot_normal_dist(mean, var)\n",
    "print(\"Distribution on plain data:\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# +\n",
    "# encrypted distribution\n",
    "def encrypted_out_distribution(eelr, enc_x_test):\n",
    "    w = eelr.weight\n",
    "    b = eelr.bias\n",
    "    data = []\n",
    "\n",
    "    enc_out = enc_x_test.dot(w) + b\n",
    "    data.append(enc_out.decrypt())\n",
    "    print(data)\n",
    "    data = torch.tensor(data)\n",
    "    mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "    plot_normal_dist(mean, var)\n",
    "    print(\"Distribution on encrypted data:\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "eelr = EncryptedLinReg(lin_reg_model)\n",
    "eelr.encrypt(context)\n",
    "encrypted_out_distribution(eelr, enc_x_train)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "703c1ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at epoch #0 is 0.0\n",
      "Loss for current iteration is 6.130923147376297e+16\n",
      "Accuracy at epoch #1 is 0.0\n",
      "Loss for current iteration is 3.503389487045734e+56\n",
      "Accuracy at epoch #2 is 0.0\n",
      "Loss for current iteration is 1.1841323265792535e+37\n",
      "Accuracy at epoch #3 is 0.0\n",
      "Loss for current iteration is 43512492022310.87\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "scale out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000012?line=18'>19</a>\u001b[0m enc_out \u001b[39m=\u001b[39m eelr\u001b[39m.\u001b[39mforward(enc_x_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000012?line=19'>20</a>\u001b[0m eelr\u001b[39m.\u001b[39mbackward(enc_x_train, enc_out, enc_x_train)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000012?line=20'>21</a>\u001b[0m eelr\u001b[39m.\u001b[39;49mupdate_parameters()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000012?line=21'>22</a>\u001b[0m t_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000012?line=22'>23</a>\u001b[0m times\u001b[39m.\u001b[39mappend(t_end \u001b[39m-\u001b[39m t_start)\n",
      "\u001b[1;32m/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb Cell 8'\u001b[0m in \u001b[0;36mEncryptedLinReg.update_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=26'>27</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou should at least run one forward iteration\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=27'>28</a>\u001b[0m \u001b[39m# update weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=28'>29</a>\u001b[0m \u001b[39m# We use a small regularization term to keep the output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=29'>30</a>\u001b[0m \u001b[39m# of the linear layer in the range of the sigmoid approximation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=31'>32</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_delta_w \u001b[39m*\u001b[39;49m (\u001b[39m1\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=32'>33</a>\u001b[0m ) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_delta_b \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/muratsaglam/Desktop/dissertation/codebase/LinearRegression/EncryptedTraining/Encrypted_Training.ipynb#ch0000007?line=34'>35</a>\u001b[0m \u001b[39m# reset gradient accumulators and iterations count\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/abstract_tensor.py:116\u001b[0m, in \u001b[0;36mAbstractTensor.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    <a href='file:///home/muratsaglam/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/abstract_tensor.py?line=114'>115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__mul__\u001b[39m(\u001b[39mself\u001b[39m, other) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAbstractTensor\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/muratsaglam/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/abstract_tensor.py?line=115'>116</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmul(other)\n",
      "File \u001b[0;32m~/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/ckksvector.py:98\u001b[0m, in \u001b[0;36mCKKSVector.mul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     <a href='file:///home/muratsaglam/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/ckksvector.py?line=95'>96</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmul\u001b[39m(\u001b[39mself\u001b[39m, other) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCKKSVector\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/muratsaglam/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/ckksvector.py?line=96'>97</a>\u001b[0m     other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_operand(other, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/muratsaglam/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/ckksvector.py?line=97'>98</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata \u001b[39m*\u001b[39;49m other\n\u001b[1;32m     <a href='file:///home/muratsaglam/Desktop/dissertation/codebase/venv/lib/python3.9/site-packages/tenseal/tensors/ckksvector.py?line=98'>99</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap(result)\n",
      "\u001b[0;31mValueError\u001b[0m: scale out of bounds"
     ]
    }
   ],
   "source": [
    "\n",
    "eelr = EncryptedLinReg(LinearRegressionModel(1, 1))\n",
    "accuracy = eelr.plain_accuracy(x_test_tensor.float(), y_test_tensor.float())\n",
    "\n",
    "print(f\"Accuracy at epoch #0 is {accuracy}\")\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "times = []\n",
    "for epoch in range(EPOCHS):\n",
    "    eelr.encrypt(context)\n",
    "\n",
    "    # if you want to keep an eye on the distribution to make sure\n",
    "    # the function approxiamation is still working fine\n",
    "    # WARNING: this operation is time consuming\n",
    "    # encrypted_out_distribution(eelr, enc_x_train)\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    enc_out = eelr.forward(enc_x_train)\n",
    "    eelr.backward(enc_x_train, enc_out, enc_x_train)\n",
    "    eelr.update_parameters()\n",
    "    t_end = time.time()\n",
    "    times.append(t_end - t_start)\n",
    "\n",
    "    eelr.decrypt()\n",
    "    accuracy = eelr.plain_accuracy(\n",
    "        x_test_tensor.flatten().float(), y_test_tensor.float()\n",
    "    )\n",
    "    print(f\"Accuracy at epoch #{epoch + 1} is {accuracy}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
    "print(f\"Final accuracy is {accuracy}\")\n",
    "\n",
    "diff_accuracy = plain_accuracy - accuracy\n",
    "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
    "if diff_accuracy < 0:\n",
    "    print(\n",
    "        \"Oh! We got a better accuracy when training on encrypted data! The noise was on our side...\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e281388417d40fdc24e67542ad3c17464e0aaf6dfc08fced3ecd5eb60e4a293"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
